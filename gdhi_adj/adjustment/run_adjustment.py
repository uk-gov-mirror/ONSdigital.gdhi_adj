"""Module for adjusting data in the gdhi_adj project."""

import os
import re

from gdhi_adj.adjustment.calc_adjustment import (
    apportion_adjustment,
    calc_midpoint_adjustment,
    calc_midpoint_val,
)
from gdhi_adj.adjustment.filter_adjustment import (
    filter_by_year,
    filter_lsoa_data,
)
from gdhi_adj.adjustment.join_adjustment import (
    join_analyst_constrained_data,
    join_analyst_unconstrained_data,
)
from gdhi_adj.adjustment.pivot_adjustment import (
    pivot_adjustment_long,
    pivot_wide_final_dataframe,
)
from gdhi_adj.adjustment.reformat_adjustment import (
    reformat_adjust_col,
    reformat_year_col,
)
from gdhi_adj.preprocess.pivot_preprocess import (
    pivot_output_long,
    pivot_wide_dataframe,
)
from gdhi_adj.utils.helpers import read_with_schema, write_with_schema
from gdhi_adj.utils.logger import GDHI_adj_logger

GDHI_adj_LOGGER = GDHI_adj_logger(__name__)
logger = GDHI_adj_LOGGER.logger


def run_adjustment(config: dict) -> None:
    """
    Run the adjustment steps for the GDHI adjustment project.

    This function performs the following steps:
    1. Load the configuration settings.
    2. Load the input data.
    3. Filter PowerBI adjustment selection data.
    4. Join analyst output with Regional Accounts data.
    5. Pivot the DataFrame to long format for manipulation.
    6. Calculate scaling factors.
    7. Calculate headroom and midpoint of time series.
    8. Calculate adjustment and distribute amongst time series.
    9. Pivot data back to wide format.
    10. Save the adjusted data.

    Args:
        config (dict): Configuration dictionary containing user settings and
        pipeline settings.
    Returns:
        None: The function does not return any value. It saves the processed
        DataFrame to a CSV file.
    """
    logger.info("Adjustment started")

    logger.info("Loading configuration settings")
    local_or_shared = config["user_settings"]["local_or_shared"]
    filepath_dict = config[f"adjustment_{local_or_shared}_settings"]
    schema_path = config["pipeline_settings"]["schema_path"]

    input_adj_file_path = (
        "C:/Users/" + os.getlogin() + filepath_dict["input_adj_file_path"]
    )
    input_constrained_file_path = (
        "C:/Users/"
        + os.getlogin()
        + filepath_dict["input_constrained_file_path"]
    )
    input_unconstrained_file_path = (
        "C:/Users/"
        + os.getlogin()
        + filepath_dict["input_unconstrained_file_path"]
    )

    match = re.search(
        r".*GDHI_Preproc_(.*?)_[^_]+\.csv", input_unconstrained_file_path
    )

    if match:
        gdhi_suffix = match.group(1) + "_"

    input_adj_schema_path = (
        schema_path + config["pipeline_settings"]["input_adj_schema_name"]
    )
    input_constrained_schema_path = (
        schema_path
        + config["pipeline_settings"]["input_constrained_schema_name"]
    )
    input_unconstrained_schema_path = (
        schema_path
        + config["pipeline_settings"]["input_unconstrained_schema_name"]
    )

    start_year = config["user_settings"]["start_year"]
    end_year = config["user_settings"]["end_year"]

    output_dir = "C:/Users/" + os.getlogin() + filepath_dict["output_dir"]
    output_adjustment_powerbi_schema_path = (
        schema_path
        + config["pipeline_settings"]["output_adjustment_powerbi_schema_path"]
    )
    output_schema_path = (
        schema_path
        + config["pipeline_settings"]["output_adjustment_schema_path"]
    )
    interim_filename = gdhi_suffix + filepath_dict.get(
        "interim_filename", None
    )
    powerbi_filename = gdhi_suffix + filepath_dict.get(
        "output_powerbi_filename", None
    )
    new_filename = gdhi_suffix + filepath_dict.get("output_filename", None)

    logger.info("Reading in data with schemas")
    df_powerbi_output = read_with_schema(
        input_adj_file_path, input_adj_schema_path
    )
    df_constrained = read_with_schema(
        input_constrained_file_path, input_constrained_schema_path
    )
    df_unconstrained = read_with_schema(
        input_unconstrained_file_path, input_unconstrained_schema_path
    )

    logger.info("Reformatting adjust and year columns.")
    df_powerbi_output = reformat_adjust_col(df_powerbi_output)

    df_powerbi_output = reformat_year_col(df_powerbi_output)

    logger.info("Filtering for data that requires adjustment.")
    df_powerbi_output = filter_lsoa_data(df_powerbi_output)

    logger.info("Joining analyst output and constrained DAP output")
    df = join_analyst_constrained_data(df_constrained, df_powerbi_output)

    logger.info("Joining analyst output and unconstrained DAP output")
    df = join_analyst_unconstrained_data(df_unconstrained, df)

    logger.info("Pivoting DataFrame long")
    df = pivot_adjustment_long(df)

    logger.info("Filtering DataFrame by year range")
    df = filter_by_year(df, start_year, end_year)

    logger.info("Calculating outlier year midpoints")
    midpoint_df = calc_midpoint_val(df)

    logger.info("Calculating adjustment values based on midpoints")
    df = calc_midpoint_adjustment(df, midpoint_df)

    logger.info("Apportioning adjustment values to all years")
    df = apportion_adjustment(df)

    logger.info("Saving interim data")
    logger.info(f"{output_dir + interim_filename}")
    df.to_csv(
        output_dir + interim_filename,
        index=False,
    )
    logger.info("Data saved successfully")

    df = df.drop(
        columns=[
            "con_gdhi",
            "midpoint",
            "midpoint_diff",
            "adjustment_val",
            "year_count",
        ]
    ).rename(columns={"adjusted_con_gdhi": "con_gdhi"})

    logger.info("Pivoting DataFrame wide for PowerBI QA")
    print(df)
    breakpoint()
    powerbi_qa_df = pivot_output_long(df, "uncon_gdhi", "con_gdhi")
    powerbi_qa_df = pivot_wide_dataframe(powerbi_qa_df)

    # Save output file with new filename if specified
    if config["user_settings"]["output_data"]:
        # Write DataFrame to CSV
        write_with_schema(
            powerbi_qa_df,
            output_adjustment_powerbi_schema_path,
            output_dir,
            powerbi_filename,
        )

    logger.info("Pivoting final DataFrame wide for exporting")
    breakpoint()
    df = pivot_wide_final_dataframe(df)

    # Save output file with new filename if specified
    if config["user_settings"]["output_data"]:
        # Write DataFrame to CSV
        write_with_schema(df, output_schema_path, output_dir, new_filename)
